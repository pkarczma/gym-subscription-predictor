{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_subscription_predictor.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCfIqN1uuzIW"
      },
      "source": [
        "# Executive Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_ID2t3oYtey"
      },
      "source": [
        "This report describes the approach taken to build a gym subscription prediction model based on the user data provided as an input. It consist of descriptions, as well as the code written in Python language used to process data, build a model, and obtain final results. First section of this report serves to access data, get familiar with it, clean it, and transform it for the model to read it. Second section explains which model was selected and how it was build, compiled, and fit. Next section presents model quality assessment approach. Then, it is shown how the test file was scored with the prepared model. Last two sections describe findings encountered during the process and limitations of selected approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZXrkeP7u_Ns"
      },
      "source": [
        "# Input Data and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBTG04EdMBF"
      },
      "source": [
        "In this section the process of analysis of the provided data is described. For this task I am going to store data in a format of so-called dataframes (DataFrame classes) from Pandas library wirtten for Python language:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKs1MxhLdFw7"
      },
      "source": [
        "import pandas as pd\n",
        "#import json"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_p7j_XmsYmm"
      },
      "source": [
        "### Access data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mvKSUs_Ca3z"
      },
      "source": [
        "The data provided for this task has been uploaded in order to automatize this report. To access data I am going to clone the files that I stored in the git repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhctCVOi0JIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83517726-cc15-414b-a420-36132b87c9a9"
      },
      "source": [
        "!git clone https://pkarczma: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@github.com/pkarczma/gym-subscription-predictor.git"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gym-subscription-predictor' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPSHXrj2CeRK"
      },
      "source": [
        "Next, one needs to read it. I am going to start with the data used for training. The data consist of CSV and JSON files, which are going to be read into two dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YudIQBVaw3OS"
      },
      "source": [
        "path = 'gym-subscription-predictor/'\n",
        "df_csv = pd.read_csv(path+'train.csv')\n",
        "df_json = pd.read_json(path+'train.json', orient='split').set_index('id')"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdZ6phmWgf0U"
      },
      "source": [
        "### Merge data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVWv0Uf9gxYz"
      },
      "source": [
        "The data for each used is split into two files: structured CSV file and unstructured JSON file. The data read from JSON file is unstructured. What we would like to achieve is one dataframe that consist of rows containing all the information about each user. To do that the data read from JSON file needs some conversion in order to extract necessary data nested inside. A new dataframe containing a list of group names for each user will be extracted. The information about the date of joining the group is skipped as this seem redundant for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrNAlGxCz0P"
      },
      "source": [
        "df_groups = pd.DataFrame(columns=['groups'])\n",
        "for i in df_json.to_dict()['groups'].items():\n",
        "  groups = ''\n",
        "  for j in i[1]['data']:\n",
        "    if len(groups) > 0:\n",
        "      groups += '|'\n",
        "    groups += j['group_name']\n",
        "  df_groups = df_groups.append({'groups': groups}, ignore_index=True)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxLBMbYiTsh"
      },
      "source": [
        "We can now merge two dataframes into one dataset containing all information about each user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyrqtetEiXkt"
      },
      "source": [
        "df = pd.concat([df_csv, df_groups], axis=1)"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1I_NrSsiLX"
      },
      "source": [
        "### Analyse and clear data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VTw4XZCoFD"
      },
      "source": [
        "In this section we are going to get familiar with the data. We can have a look at some information about the dataframe and its columns, as well as look at the first few rows of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33u8oZZk_BMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "3933c6b6-917b-4a1d-ae28-2c8a1da5b15f"
      },
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4000 entries, 0 to 3999\n",
            "Data columns (total 17 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   user_id                   4000 non-null   int64  \n",
            " 1   target                    4000 non-null   int64  \n",
            " 2   name                      3982 non-null   object \n",
            " 3   sex                       3616 non-null   object \n",
            " 4   dob                       3606 non-null   object \n",
            " 5   location                  4000 non-null   object \n",
            " 6   location_population       4000 non-null   int64  \n",
            " 7   location_from             4000 non-null   object \n",
            " 8   location_from_population  4000 non-null   int64  \n",
            " 9   occupation                4000 non-null   object \n",
            " 10  hobbies                   3320 non-null   object \n",
            " 11  daily_commute             3595 non-null   float64\n",
            " 12  friends_number            4000 non-null   int64  \n",
            " 13  relationship_status       3607 non-null   object \n",
            " 14  education                 3592 non-null   float64\n",
            " 15  credit_card_type          3572 non-null   object \n",
            " 16  groups                    4000 non-null   object \n",
            "dtypes: float64(2), int64(5), object(10)\n",
            "memory usage: 531.4+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>target</th>\n",
              "      <th>name</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>location</th>\n",
              "      <th>location_population</th>\n",
              "      <th>location_from</th>\n",
              "      <th>location_from_population</th>\n",
              "      <th>occupation</th>\n",
              "      <th>hobbies</th>\n",
              "      <th>daily_commute</th>\n",
              "      <th>friends_number</th>\n",
              "      <th>relationship_status</th>\n",
              "      <th>education</th>\n",
              "      <th>credit_card_type</th>\n",
              "      <th>groups</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Halina</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1982-08-07</td>\n",
              "      <td>Piastów</td>\n",
              "      <td>22732</td>\n",
              "      <td>Piastów</td>\n",
              "      <td>22732</td>\n",
              "      <td>Teaching professionals</td>\n",
              "      <td>Fitness</td>\n",
              "      <td>46.0</td>\n",
              "      <td>196</td>\n",
              "      <td>Single</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Visa</td>\n",
              "      <td>Let's excercise together and lose a few kilo q...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Eustachy</td>\n",
              "      <td>male</td>\n",
              "      <td>1971-10-28</td>\n",
              "      <td>Sokółka</td>\n",
              "      <td>18331</td>\n",
              "      <td>Sokółka</td>\n",
              "      <td>18331</td>\n",
              "      <td>General and keyboard clerks</td>\n",
              "      <td>LARPing,Foreign language learning,Netball</td>\n",
              "      <td>55.0</td>\n",
              "      <td>243</td>\n",
              "      <td>Single</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Tom Cruise group|Babysitters (Sokółka)|Work ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Egon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2000-07-05</td>\n",
              "      <td>Łaskarzew</td>\n",
              "      <td>4879</td>\n",
              "      <td>Łaskarzew</td>\n",
              "      <td>4879</td>\n",
              "      <td>Protective services workers</td>\n",
              "      <td>Bodybuilding,Kabaddi</td>\n",
              "      <td>90.0</td>\n",
              "      <td>191</td>\n",
              "      <td>In relationship</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Polish wildlife - best places|Politics and pol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Eulalia</td>\n",
              "      <td>female</td>\n",
              "      <td>1992-06-10</td>\n",
              "      <td>Bydgoszcz</td>\n",
              "      <td>352313</td>\n",
              "      <td>Bydgoszcz</td>\n",
              "      <td>352313</td>\n",
              "      <td>Customer services clerks</td>\n",
              "      <td>Badminton</td>\n",
              "      <td>88.0</td>\n",
              "      <td>164</td>\n",
              "      <td>In relationship</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Visa</td>\n",
              "      <td>The Aspiring Writer|Nutrition &amp; food advices|G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Hilary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1975-01-09</td>\n",
              "      <td>Osieczna</td>\n",
              "      <td>2322</td>\n",
              "      <td>Poznań</td>\n",
              "      <td>538633</td>\n",
              "      <td>Refuse workers and other elementary workers</td>\n",
              "      <td>Fitness,Embroidery,Lacemaking</td>\n",
              "      <td>40.0</td>\n",
              "      <td>119</td>\n",
              "      <td>Married with kids</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The ultimate house and electro group|Pirates o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  ...                                             groups\n",
              "0        0  ...  Let's excercise together and lose a few kilo q...\n",
              "1        1  ...  Tom Cruise group|Babysitters (Sokółka)|Work ab...\n",
              "2        2  ...  Polish wildlife - best places|Politics and pol...\n",
              "3        3  ...  The Aspiring Writer|Nutrition & food advices|G...\n",
              "4        4  ...  The ultimate house and electro group|Pirates o...\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ED8BDNPkGK"
      },
      "source": [
        "From this overview we can extract many useful information. We have 4000 users and 16 columns describing each user. There are some columns with empty cells, but we are going to deal with it later. At this point it would be useful to evaluate which part of the data isn't necessary for the model. I am going to assume that the following columns are not indicating an initial interest in gym subscription:\n",
        "* name\n",
        "* location_population\n",
        "* location_from_population\n",
        "* daily_commute\n",
        "* credit_card_type\n",
        "\n",
        "As a result I am going to drop them from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Var-YiBKPAwQ"
      },
      "source": [
        "df = df.drop(columns=['name', 'location_population', 'location_from_population', 'daily_commute', 'credit_card_type'])"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NM9TAI6EPtI"
      },
      "source": [
        "Afterwards, we can focus on the missing values that are still in the remaining data. We can count the number of missing values in each of the remaining columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ4C8MHODTMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032c3222-3d10-4b16-d848-a04b2cace550"
      },
      "source": [
        "df.isnull().sum(axis = 0)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "user_id                  0\n",
              "target                   0\n",
              "sex                    384\n",
              "dob                    394\n",
              "location                 0\n",
              "location_from            0\n",
              "occupation               0\n",
              "hobbies                680\n",
              "friends_number           0\n",
              "relationship_status    393\n",
              "education              408\n",
              "groups                   0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYdofSPOC8dM"
      },
      "source": [
        "There are still some missing values in data in several columns. I am going to use a different approach depending on the column with the missing values. The following procedure will be applied:\n",
        "* 'user_id' / 'target' / 'location' / 'location_from' / 'occupation' / 'friends_number': no missing values, columns are useful, nothing changes\n",
        "* 'education': fill missing falues with a median of a column\n",
        "* 'hobbies': I assume that empty value means that a user has no hobbies, so I will fill missing values with empty string\n",
        "\n",
        "For the remaining data with missing values it is problematic to deduce how replace it. One can also notice that the fraction of missing values to all values isn't too large - it is around 10% for each of the remaining columns with missing values. Consequently, the rows with at least one missing calue will be dropped from the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAcLq_a0_Rtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d6c3c8-e0ca-432d-fdf6-b5be0ef1265d"
      },
      "source": [
        "df['hobbies'] = df['hobbies'].fillna('')\n",
        "df['education'] = df['education'].fillna(df['education'].median())\n",
        "df = df.dropna()\n",
        "df.info()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2945 entries, 1 to 3999\n",
            "Data columns (total 12 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   user_id              2945 non-null   int64  \n",
            " 1   target               2945 non-null   int64  \n",
            " 2   sex                  2945 non-null   object \n",
            " 3   dob                  2945 non-null   object \n",
            " 4   location             2945 non-null   object \n",
            " 5   location_from        2945 non-null   object \n",
            " 6   occupation           2945 non-null   object \n",
            " 7   hobbies              2945 non-null   object \n",
            " 8   friends_number       2945 non-null   int64  \n",
            " 9   relationship_status  2945 non-null   object \n",
            " 10  education            2945 non-null   float64\n",
            " 11  groups               2945 non-null   object \n",
            "dtypes: float64(1), int64(3), object(8)\n",
            "memory usage: 299.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDV-Kfvpk8Dj"
      },
      "source": [
        "As a result, we removed around 25% of all rows and we are left with 2945 users. I assume that this is a sufficient number of entries for the model. Now the data is clean and ready for the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDtd4CLXr5T0"
      },
      "source": [
        "### Transform data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5LNwLGsE-B"
      },
      "source": [
        "In order to prepare data for the model we need to convert it to the proper format. The following code will convert non-numeric data to categories so that is it easier for the model to read it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnHh-uOtLFq"
      },
      "source": [
        "df['sex'] = df['sex'].astype('category').cat.codes\n",
        "df['location'] = df['location'].astype('category').cat.codes\n",
        "df['location_from'] = df['location_from'].astype('category').cat.codes\n",
        "df['occupation'] = df['occupation'].astype('category').cat.codes\n",
        "df['relationship_status'] = df['relationship_status'].astype('category').cat.codes"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbhk_WEBLkyK"
      },
      "source": [
        "For the date of birth, I assume there is no need to keep the exact date - having just a year of birth should be enough for the model. I will drop the day and month information from 'dob' column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnV7mBe_L1YY"
      },
      "source": [
        "df['dob'] = pd.DatetimeIndex(df['dob']).year"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ybXqKhru2E0"
      },
      "source": [
        "For the 'hobbies' column the best way is to extract dummies for each row and split it into columns with values of 0 or 1 indicating interest (or lack of interest) in a particular hobby. An additional 'hobby_' prefix will indicate that this column represents a hobby, but also to make sure that none of the column names are overlapping with the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsfVaCK7pJwi"
      },
      "source": [
        "df = pd.concat([df.drop('hobbies', axis=1), df['hobbies'].str.get_dummies(sep=',').add_prefix('hobby_')], axis=1)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwtaiam9EFXz"
      },
      "source": [
        "I am going to apply a similar procedure to the 'groups' column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Zq8C8HEMEA"
      },
      "source": [
        "df = pd.concat([df.drop('groups', axis=1), df['groups'].str.get_dummies(sep='|').add_prefix('group_')], axis=1)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEBuPt4fvZoZ"
      },
      "source": [
        "At this point the data contains only numbers, there are no missing values, and it is prepared for the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KuLSWfovvSj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "d981eceb-7577-4059-bca3-d6e5c8b2c62c"
      },
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2945 entries, 1 to 3999\n",
            "Columns: 3521 entries, user_id to group_muscle, strength  and vascularity - What every bodybuilder should know\n",
            "dtypes: float64(1), int16(2), int64(3515), int8(3)\n",
            "memory usage: 79.0 MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>target</th>\n",
              "      <th>sex</th>\n",
              "      <th>dob</th>\n",
              "      <th>location</th>\n",
              "      <th>location_from</th>\n",
              "      <th>occupation</th>\n",
              "      <th>friends_number</th>\n",
              "      <th>relationship_status</th>\n",
              "      <th>education</th>\n",
              "      <th>hobby_3D printing</th>\n",
              "      <th>hobby_Acting</th>\n",
              "      <th>hobby_Air sports</th>\n",
              "      <th>hobby_Amateur radio</th>\n",
              "      <th>hobby_Archery</th>\n",
              "      <th>hobby_Astronomy</th>\n",
              "      <th>hobby_BASE jumping</th>\n",
              "      <th>hobby_Backpacking</th>\n",
              "      <th>hobby_Badminton</th>\n",
              "      <th>hobby_Baseball</th>\n",
              "      <th>hobby_Basketball</th>\n",
              "      <th>hobby_Beekeeping</th>\n",
              "      <th>hobby_Bird watching</th>\n",
              "      <th>hobby_Blacksmithing</th>\n",
              "      <th>hobby_Board games</th>\n",
              "      <th>hobby_Board sports</th>\n",
              "      <th>hobby_Bodybuilding</th>\n",
              "      <th>hobby_Book restoration</th>\n",
              "      <th>hobby_Brazilian jiu-jitsu</th>\n",
              "      <th>hobby_Cabaret</th>\n",
              "      <th>hobby_Calligraphy</th>\n",
              "      <th>hobby_Candle making</th>\n",
              "      <th>hobby_Cardio</th>\n",
              "      <th>hobby_Coffee roasting</th>\n",
              "      <th>hobby_Coloring</th>\n",
              "      <th>hobby_Community</th>\n",
              "      <th>hobby_Computer programming</th>\n",
              "      <th>hobby_Cooking</th>\n",
              "      <th>hobby_Cosplaying</th>\n",
              "      <th>hobby_Creative writing</th>\n",
              "      <th>...</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Sopot)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Sosnowiec)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Swarzędz)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Szczecin)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Słubice)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Słupsk)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Tarnów)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Tczew)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Terespol)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Tomaszów Mazowiecki)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Toruń)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Tuchola)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Tychy)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Warszawa)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Warta)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Wasilków)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Wolbrom)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Wrocław)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Włocławek)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Zabrze)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Zator)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Zduńska Wola)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Zielona Góra)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Ząbki)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Łomża)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Łódź)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Śrem)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Świdnik)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Świętochłowice)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Żagań)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Żary)</th>\n",
              "      <th>group_Work abroad - join to find well paid work and enjoy the experience (Żyrardów)</th>\n",
              "      <th>group_World Weightlifting Championships</th>\n",
              "      <th>group_Xbox , Playstation, Wii - console fans</th>\n",
              "      <th>group_Young, fit and healthy - active livestyle = healthy lifestyle.</th>\n",
              "      <th>group_ZTM Warsaw</th>\n",
              "      <th>group_alternative medicine - Hypnosis and bioenergotheraphy</th>\n",
              "      <th>group_ham cooker - recipes</th>\n",
              "      <th>group_instrumental music - the unheard and undiscovered</th>\n",
              "      <th>group_muscle, strength  and vascularity - What every bodybuilder should know</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1971</td>\n",
              "      <td>400</td>\n",
              "      <td>415</td>\n",
              "      <td>15</td>\n",
              "      <td>243</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1992</td>\n",
              "      <td>46</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>164</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1981</td>\n",
              "      <td>465</td>\n",
              "      <td>490</td>\n",
              "      <td>22</td>\n",
              "      <td>117</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1978</td>\n",
              "      <td>81</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>224</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1985</td>\n",
              "      <td>465</td>\n",
              "      <td>490</td>\n",
              "      <td>26</td>\n",
              "      <td>187</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3521 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  ...  group_muscle, strength  and vascularity - What every bodybuilder should know\n",
              "1        1  ...                                                  0                           \n",
              "3        3  ...                                                  0                           \n",
              "6        6  ...                                                  0                           \n",
              "7        7  ...                                                  0                           \n",
              "9        9  ...                                                  0                           \n",
              "\n",
              "[5 rows x 3521 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN7OlPw_vFjh"
      },
      "source": [
        "# Model Selection and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9lH1fUWppzz"
      },
      "source": [
        "In this section I am going to build a model that will be used for predictions. For this task I am going to use Keras interface from the TensorFlow library, as well as some other libraries necessary for data validation and plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jvKQBBEqC7g"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPSUlIUzXcf"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Ozjnawyc97"
      },
      "source": [
        "For the model training and testing the dataset will be split into two subsets:\n",
        "* 80% of the data will be used for training\n",
        "* 20% od the remaining data will be used for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVIGMXCx3Og"
      },
      "source": [
        "train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "test_dataset = df.drop(train_dataset.index)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46_y2c4y2x7"
      },
      "source": [
        "I am going to separate labels from features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EGNKJrEzLTw"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "train_labels = train_features.pop('target')\n",
        "test_labels = test_features.pop('target')"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhDAaP8uz31-"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIMu-S6Pz6Tr"
      },
      "source": [
        "Now it is time to build a model. The selected model will be a regression-based neural network consisting of several input, hidden, and output layers. It will use existing data prepared in the previous section as an input in order to create predictions of the desired variable.\n",
        "\n",
        "This model prefers to have the input data normalized in a specific way. Thus, we need to create a normalization layer that is adapted to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9aKQTJD22Mu"
      },
      "source": [
        "normalizer = layers.experimental.preprocessing.Normalization()\n",
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXO3e9XE3rCj"
      },
      "source": [
        "Afterwards, we can build a fully-connected model consisting of a sequential stack of layers, where first layer is a normalization layer, then there are hidden layers using a rectified linear unit as an activation function, while the last output layer is using a sigmoid function in order to ensure our network output is between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaaF8Rf537FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d11269-ca9f-4568-cc9c-3c4eb67a82b5"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(3520, input_dim=3520, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "normalization_2 (Normalizati (None, 3520)              7041      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3520)              12393920  \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 3521      \n",
            "=================================================================\n",
            "Total params: 12,404,482\n",
            "Trainable params: 12,397,441\n",
            "Non-trainable params: 7,041\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmL5utb5kDg"
      },
      "source": [
        "### Compile and fit model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uf79bLo5IWr"
      },
      "source": [
        "Next step is to compile the model. We need to specify the loss function to use for weights evaluation. As this is a binary classification problem we are going to use 'binary_crossentropy' as a loss function. We will classify the model using the collected accuracy value. We are also going to use Adam optimization algorithm for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adPXX-CJ5S_V"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISqfsuHz5bpC"
      },
      "source": [
        "Then, we can fit the model providing different settings that can be adjusted for the model efficacy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zscy8dS56Ces"
      },
      "source": [
        "history = model.fit(\n",
        "    # Data to be used for training\n",
        "    train_features, train_labels,\n",
        "    # Number of epochs\n",
        "    epochs=5,\n",
        "    # Suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSiw_3tm6f4u"
      },
      "source": [
        "We can have a look at the last few epochs of the training of the model in order to see if everything works well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuKOm3rE6rkP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0e0a9587-1d42-46de-91c3-cd2a071d15e7"
      },
      "source": [
        "# Show history in the last few epochs\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.949993</td>\n",
              "      <td>0.773885</td>\n",
              "      <td>1.022934</td>\n",
              "      <td>0.832627</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.079435</td>\n",
              "      <td>0.968153</td>\n",
              "      <td>0.786424</td>\n",
              "      <td>0.858051</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.018477</td>\n",
              "      <td>0.995223</td>\n",
              "      <td>0.709390</td>\n",
              "      <td>0.860169</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.009324</td>\n",
              "      <td>0.998408</td>\n",
              "      <td>0.721772</td>\n",
              "      <td>0.858051</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.006124</td>\n",
              "      <td>0.999469</td>\n",
              "      <td>0.722202</td>\n",
              "      <td>0.860169</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  accuracy  val_loss  val_accuracy  epoch\n",
              "0  0.949993  0.773885  1.022934      0.832627      0\n",
              "1  0.079435  0.968153  0.786424      0.858051      1\n",
              "2  0.018477  0.995223  0.709390      0.860169      2\n",
              "3  0.009324  0.998408  0.721772      0.858051      3\n",
              "4  0.006124  0.999469  0.722202      0.860169      4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQAyb8qnvJvn"
      },
      "source": [
        "# Model Quality Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlPqC2saENSO"
      },
      "source": [
        "In order to assess the quality of the model we are going to use the part of the dataset that hasn't been provided to the model yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q1QhngttOEm"
      },
      "source": [
        "test_predictions = model.predict(test_features)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf74onCCEk5B"
      },
      "source": [
        "Now we can see what is the fraction of correct prediction by comparing it to the true labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGV4JsCxEgjo"
      },
      "source": [
        "correct = sum(i == j for i, j in zip(np.around(test_predictions), test_labels))[0]\n",
        "print(correct / len(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BNc8dKRE13F"
      },
      "source": [
        "One can see that over 80% of predictions are correct. Even better way to look at the results is to create a confuction matrix showing the fraction of correct and incorrect predictions in each class (in this case it will be '0' and '1' as this is a binary classification)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFoaG3SOEh0S"
      },
      "source": [
        "cm = confusion_matrix(test_labels, np.around(test_predictions))\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Normalized confusion matrix')\n",
        "plt.colorbar()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, \"{0:0.2f}\".format(cm[i, j]),\n",
        "        horizontalalignment=\"center\",\n",
        "        color=\"white\" if cm[i, j] > thresh else \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEJGv9UoFPMk"
      },
      "source": [
        "One can see that over 90% of labels marked as '0' are correctly identified. For labels marked as '1' the model doesn't work that well and predicts correctly only around 28% of all cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YTPzIsrI9uY"
      },
      "source": [
        "# Scoring Test File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBVoPB8Jns7"
      },
      "source": [
        "In this section the model will be used to produce target variable on the data stored in test.csv and test.json files. First, we need to read the file and transform it for the model in a similar way as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ6WlqqZKGjW"
      },
      "source": [
        "df_model = df.copy()\n",
        "df_csv = pd.read_csv(path+'test.csv')\n",
        "df_json = pd.read_json(path+'test.json', orient='split').set_index('id')\n",
        "df_groups = pd.DataFrame(columns=['groups'])\n",
        "for i in df_json.to_dict()['groups'].items():\n",
        "  groups = ''\n",
        "  for j in i[1]['data']:\n",
        "    if len(groups) > 0:\n",
        "      groups += '|'\n",
        "    groups += j['group_name']\n",
        "  df_groups = df_groups.append({'groups': groups}, ignore_index=True)\n",
        "df_groups.head()\n",
        "df = pd.concat([df_csv, df_groups], axis=1)\n",
        "df = df.drop(columns=['target', 'name', 'location_population', 'location_from_population', 'daily_commute', 'credit_card_type'])\n",
        "df['hobbies'] = df['hobbies'].fillna('')\n",
        "df['education'] = df['education'].fillna(df['education'].median())\n",
        "df = df.dropna()\n",
        "df['sex'] = df['sex'].astype('category').cat.codes\n",
        "df['location'] = df['location'].astype('category').cat.codes\n",
        "df['location_from'] = df['location_from'].astype('category').cat.codes\n",
        "df['occupation'] = df['occupation'].astype('category').cat.codes\n",
        "df['relationship_status'] = df['relationship_status'].astype('category').cat.codes\n",
        "df['dob'] = pd.DatetimeIndex(df['dob']).year\n",
        "df = pd.concat([df.drop('hobbies', axis=1), df['hobbies'].str.get_dummies(sep=',').add_prefix('hobby_')], axis=1)\n",
        "df = pd.concat([df.drop('groups', axis=1), df['groups'].str.get_dummies(sep='|').add_prefix('group_')], axis=1)\n",
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_W4qf5LM9TP"
      },
      "source": [
        "Now we need to add columns that are missing from this dataframe and remove additional columns that didn't exist previously in order to have exactly the same set of columns as in the dataframe used for the model building. New columns will be filled with '0'. Additional column will be dropped because the model doesn't know what to do with hobbies or groups that didn't exist in the fitting data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbcppS4OSWd"
      },
      "source": [
        "missing_col = df_model.columns.difference(df.columns)\n",
        "for col in missing_col:\n",
        "  df[col] = 0\n",
        "additional_col = df.columns.difference(df_model.columns)\n",
        "df.drop(labels=additional_col.tolist(), axis=1, inplace=True)\n",
        "df.drop(labels='target', axis=1, inplace=True)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOeDe7ZAQvSD"
      },
      "source": [
        "Now we can use the model to predict target value for the test dataset. As a result we get a list of probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo4NpOUtQ11S"
      },
      "source": [
        "test_features = df.copy()\n",
        "test_predictions = model.predict(test_features)\n",
        "print(test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87JsqknSRqgb"
      },
      "source": [
        "We can finally prepare a scored test file. For the rows that we skipped in the prediction phase due to missing information we are going to assume the target variable as '0'. For the rest of the users we will use predicted probability and set a target value by rounding probability to the nearest integer value. Then, we save the output file as a CSV in a desired format consisting of 3 columns: 'user_id', 'probability_of_one', and 'target'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFJ6PEB4SD8W"
      },
      "source": [
        "df_score = pd.DataFrame(columns=['user_id', 'probability_of_one', 'target'])\n",
        "user_pred = 0\n",
        "for user in range(df_csv.shape[0]):\n",
        "  if user in df['user_id']:\n",
        "    df_score =df_score.append({'user_id': user, 'probability_of_one': test_predictions[user_pred][0], 'target': np.around(test_predictions[user_pred][0])}, ignore_index=True)\n",
        "    user_pred += 1\n",
        "  else:\n",
        "    df_score =df_score.append({'user_id': user, 'probability_of_one': 0., 'target': 0.}, ignore_index=True)\n",
        "df_score = df_score.astype({'user_id': int, 'target': int})\n",
        "df_score.head()\n",
        "df_score.to_csv('test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxaM9uUmvMzi"
      },
      "source": [
        "# Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw54fzzcvOLg"
      },
      "source": [
        "# Limitations of the Approach"
      ]
    }
  ]
}