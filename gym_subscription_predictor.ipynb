{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_subscription_predictor.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCfIqN1uuzIW"
      },
      "source": [
        "# Executive Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZXrkeP7u_Ns"
      },
      "source": [
        "# Input Data and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_p7j_XmsYmm"
      },
      "source": [
        "### Access data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgYjTYBeCXhn"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj27OvRMxenD"
      },
      "source": [
        "import pandas as pd\n",
        "import json"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mvKSUs_Ca3z"
      },
      "source": [
        "Clone files stored in the git repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhctCVOi0JIU"
      },
      "source": [
        "!git clone https://pkarczma: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@github.com/pkarczma/gym-subscription-predictor.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPSHXrj2CeRK"
      },
      "source": [
        "Read CSV and JSON files with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YudIQBVaw3OS"
      },
      "source": [
        "path = 'gym-subscription-predictor/'\n",
        "df_csv = pd.read_csv(path+'train.csv')\n",
        "df_json = pd.read_json(path+'train.json', orient='split').set_index('id')"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plx3avQ4Chuk"
      },
      "source": [
        "The data in JSON file needs some conversion in order to extract necessary data nested inside. A new dataframe containing only information about group names will be extracted and merged with the information from CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrNAlGxCz0P"
      },
      "source": [
        "df_groups = pd.DataFrame(columns=['groups'])\n",
        "for i in df_json.to_dict()['groups'].items():\n",
        "  groups = ''\n",
        "  for j in i[1]['data']:\n",
        "    if len(groups) > 0:\n",
        "      groups += '|'\n",
        "    groups += j['group_name']\n",
        "  df_groups = df_groups.append({'groups': groups}, ignore_index=True)\n",
        "df_groups.head()\n",
        "df = pd.concat([df_csv, df_groups], axis=1)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1I_NrSsiLX"
      },
      "source": [
        "### Analyse and clear data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VTw4XZCoFD"
      },
      "source": [
        "Get familiar with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33u8oZZk_BMe"
      },
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ED8BDNPkGK"
      },
      "source": [
        "There are some columns that seem unnnecessary for our model. We will drop them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Var-YiBKPAwQ"
      },
      "source": [
        "df = df.drop(columns=['name', 'location_population', 'location_from_population', 'daily_commute', 'credit_card_type'])"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NM9TAI6EPtI"
      },
      "source": [
        "Count the number of missing values in each of the remaining columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ4C8MHODTMO"
      },
      "source": [
        "df.isnull().sum(axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYdofSPOC8dM"
      },
      "source": [
        "There are some NaN values in data in several columns. We need to use a different approach depending on the column with the missing values. The following procedure will be applied:\n",
        "* 'user_id' / 'target' / 'location' / 'occupation' / 'friends_number': no missing values, columns are useful, nothing changes\n",
        "* 'name' / 'location_population' / 'location_from' / 'location_from_population': a few missing values, this column isn't necessary for model prediction so it will be dropped\n",
        "* 'education': fill missing falues with a median of a column\n",
        "* 'hobbies': fill missing values with empty string\n",
        "\n",
        "For the remaining data with missing values it is problematic to replace it. Thus, the rows with at least one missing calue will be dropped from the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAcLq_a0_Rtr"
      },
      "source": [
        "df['hobbies'] = df['hobbies'].fillna('')\n",
        "df['education'] = df['education'].fillna(df['education'].median())\n",
        "df = df.dropna()\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDV-Kfvpk8Dj"
      },
      "source": [
        "As a result, we removed around 25% of all rows, but now the data is clean and ready for the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDtd4CLXr5T0"
      },
      "source": [
        "### Transform data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5LNwLGsE-B"
      },
      "source": [
        "In order to prepare data for the model we need to convert it to the proper format. The following code will convert data to categories so that is it easier for the model to read it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnHh-uOtLFq"
      },
      "source": [
        "df['sex'] = df['sex'].astype('category').cat.codes\n",
        "df['location'] = df['location'].astype('category').cat.codes\n",
        "df['location_from'] = df['location_from'].astype('category').cat.codes\n",
        "df['occupation'] = df['occupation'].astype('category').cat.codes\n",
        "df['relationship_status'] = df['relationship_status'].astype('category').cat.codes"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbhk_WEBLkyK"
      },
      "source": [
        "For the date of birth, I assume there is no need to keep the exact date - having just a year of birth should be enough for the model. I will drop the day and month information from 'dob' column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnV7mBe_L1YY"
      },
      "source": [
        "df['dob'] = pd.DatetimeIndex(df['dob']).year"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ybXqKhru2E0"
      },
      "source": [
        "For the 'hobbies' column the best way is to get dummies for each value and split it into several columns with numbers 0 and 1 indicating interest (or lack of interest) in a particular hobby. An additional 'hobby_' prefix will indicate that this column represents a hobby, but also to make sure that none of the column names are overlapping with the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsfVaCK7pJwi"
      },
      "source": [
        "df = pd.concat([df.drop('hobbies', axis=1), df['hobbies'].str.get_dummies(sep=',').add_prefix('hobby_')], axis=1)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwtaiam9EFXz"
      },
      "source": [
        "Let's do the same for the 'groups' column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Zq8C8HEMEA"
      },
      "source": [
        "df = pd.concat([df.drop('groups', axis=1), df['groups'].str.get_dummies(sep='|').add_prefix('group_')], axis=1)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEBuPt4fvZoZ"
      },
      "source": [
        "At this point the data contain only numbers, there are no missing values, and it is prepared for the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KuLSWfovvSj"
      },
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN7OlPw_vFjh"
      },
      "source": [
        "# Model Selection and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPSUlIUzXcf"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Ozjnawyc97"
      },
      "source": [
        "For the model training and testing the dataset will be split into two subsets:\n",
        "* 80% of the data will be used for training\n",
        "* 20% od the remaining data will be used for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVIGMXCx3Og"
      },
      "source": [
        "train_dataset = df.sample(frac=0.8, random_state=0)\n",
        "test_dataset = df.drop(train_dataset.index)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46_y2c4y2x7"
      },
      "source": [
        "Afterwards, let's separate labels from features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EGNKJrEzLTw"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "train_labels = train_features.pop('target')\n",
        "test_labels = test_features.pop('target')"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhDAaP8uz31-"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIMu-S6Pz6Tr"
      },
      "source": [
        "Now it is time to build a model. For this task I am going to use Keras interface from the TensorFlow library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTr6qm600tfd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaCPssnG1noS"
      },
      "source": [
        "The selected model will be a regression-based neural network consisting of several input, hidden, and output layers. It will use existing data prepared in the previous section as an input in order to create predictions of the desired variable.\n",
        "\n",
        "This model prefers to have the input data normalized in a specific way. Thus, we need to create a normalization layer that is adapted to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9aKQTJD22Mu"
      },
      "source": [
        "normalizer = layers.experimental.preprocessing.Normalization()\n",
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXO3e9XE3rCj"
      },
      "source": [
        "Afterwards, we can build a fully-connected model consisting of a sequential stack of layers, where first layers are using a rectified linear unit activation function, while the output layer is using a sigmoid function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaaF8Rf537FB"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(3520, input_dim=3520, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmL5utb5kDg"
      },
      "source": [
        "### Compile and fit model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uf79bLo5IWr"
      },
      "source": [
        "Next step is to compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adPXX-CJ5S_V"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISqfsuHz5bpC"
      },
      "source": [
        "Then, we can fit the model providing different settings that can be adjusted for the model efficacy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zscy8dS56Ces"
      },
      "source": [
        "history = model.fit(\n",
        "    # Data to be used for training\n",
        "    train_features, train_labels,\n",
        "    # Number of epochs\n",
        "    epochs=5,\n",
        "    # Suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSiw_3tm6f4u"
      },
      "source": [
        "We can have a look at the last few epochs of the training of the model in order to see if everything works well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuKOm3rE6rkP"
      },
      "source": [
        "# Show history in the last few epochs\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQAyb8qnvJvn"
      },
      "source": [
        "# Model Quality Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlPqC2saENSO"
      },
      "source": [
        "In order to assess the quality of the model we are going to use the part of the dataset that hasn't been provided to the model yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q1QhngttOEm"
      },
      "source": [
        "test_predictions = model.predict(test_features)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf74onCCEk5B"
      },
      "source": [
        "Now we can see what is the fraction of correct prediction by comparing it to the true labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGV4JsCxEgjo"
      },
      "source": [
        "correct = sum(i == j for i, j in zip(np.around(test_predictions), test_labels))[0]\n",
        "print(correct / len(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BNc8dKRE13F"
      },
      "source": [
        "One can see that over 80% of predictions are correct. Even better way to look at the results is to create a confuction matrix showing the fraction of correct and incorrect predictions in each class (in this case it will be '0' and '1' as this is a binary classification)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFoaG3SOEh0S"
      },
      "source": [
        "cm = confusion_matrix(test_labels, np.around(test_predictions))\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Normalized confusion matrix')\n",
        "plt.colorbar()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, \"{0:0.2f}\".format(cm[i, j]),\n",
        "        horizontalalignment=\"center\",\n",
        "        color=\"white\" if cm[i, j] > thresh else \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEJGv9UoFPMk"
      },
      "source": [
        "One can see that over 90% of labels marked as '0' are correctly identified. For labels marked as '1' the model doesn't work that well and predicts correctly only around 28% of all cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YTPzIsrI9uY"
      },
      "source": [
        "# Scoring Test File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBVoPB8Jns7"
      },
      "source": [
        "In this section the model will be used to produce target variable on the data stored in test.csv and test.json files. First, we need to read the file and transform it for the model in a similar way as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ6WlqqZKGjW"
      },
      "source": [
        "df_model = df.copy()\n",
        "df_csv = pd.read_csv(path+'test.csv')\n",
        "df_json = pd.read_json(path+'test.json', orient='split').set_index('id')\n",
        "df_groups = pd.DataFrame(columns=['groups'])\n",
        "for i in df_json.to_dict()['groups'].items():\n",
        "  groups = ''\n",
        "  for j in i[1]['data']:\n",
        "    if len(groups) > 0:\n",
        "      groups += '|'\n",
        "    groups += j['group_name']\n",
        "  df_groups = df_groups.append({'groups': groups}, ignore_index=True)\n",
        "df_groups.head()\n",
        "df = pd.concat([df_csv, df_groups], axis=1)\n",
        "df = df.drop(columns=['target', 'name', 'location_population', 'location_from_population', 'daily_commute', 'credit_card_type'])\n",
        "df['hobbies'] = df['hobbies'].fillna('')\n",
        "df['education'] = df['education'].fillna(df['education'].median())\n",
        "df = df.dropna()\n",
        "df['sex'] = df['sex'].astype('category').cat.codes\n",
        "df['location'] = df['location'].astype('category').cat.codes\n",
        "df['location_from'] = df['location_from'].astype('category').cat.codes\n",
        "df['occupation'] = df['occupation'].astype('category').cat.codes\n",
        "df['relationship_status'] = df['relationship_status'].astype('category').cat.codes\n",
        "df['dob'] = pd.DatetimeIndex(df['dob']).year\n",
        "df = pd.concat([df.drop('hobbies', axis=1), df['hobbies'].str.get_dummies(sep=',').add_prefix('hobby_')], axis=1)\n",
        "df = pd.concat([df.drop('groups', axis=1), df['groups'].str.get_dummies(sep='|').add_prefix('group_')], axis=1)\n",
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_W4qf5LM9TP"
      },
      "source": [
        "Now we need to add columns that are missing from this dataframe and remove additional columns that didn't exist previously in order to have exactly the same set of columns as in the dataframe used for the model building. New columns will be filled with '0'. Additional column will be dropped because the model doesn't know what to do with hobbies or groups that didn't exist in the fitting data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbcppS4OSWd"
      },
      "source": [
        "missing_col = df_model.columns.difference(df.columns)\n",
        "for col in missing_col:\n",
        "  df[col] = 0\n",
        "additional_col = df.columns.difference(df_model.columns)\n",
        "df.drop(labels=additional_col.tolist(), axis=1, inplace=True)\n",
        "df.drop(labels='target', axis=1, inplace=True)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOeDe7ZAQvSD"
      },
      "source": [
        "Now we can use the model to predict target value for the test dataset. As a result we get a list of probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo4NpOUtQ11S"
      },
      "source": [
        "test_features = df.copy()\n",
        "test_predictions = model.predict(test_features)\n",
        "print(test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87JsqknSRqgb"
      },
      "source": [
        "We can finally prepare a scored test file. For the rows that we skipped in the prediction phase due to missing information we are going to assume the target variable as '0'. For the rest of the users we will use predicted probability and set a target value by rounding probability to the nearest integer value. Then, we save the output file as a CSV in a desired format consisting of 3 columns: 'user_id', 'probability_of_one', and 'target'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFJ6PEB4SD8W"
      },
      "source": [
        "df_score = pd.DataFrame(columns=['user_id', 'probability_of_one', 'target'])\n",
        "user_pred = 0\n",
        "for user in range(df_csv.shape[0]):\n",
        "  if user in df['user_id']:\n",
        "    df_score =df_score.append({'user_id': user, 'probability_of_one': test_predictions[user_pred][0], 'target': np.around(test_predictions[user_pred][0])}, ignore_index=True)\n",
        "    user_pred += 1\n",
        "  else:\n",
        "    df_score =df_score.append({'user_id': user, 'probability_of_one': 0., 'target': 0.}, ignore_index=True)\n",
        "df_score = df_score.astype({'user_id': int, 'target': int})\n",
        "df_score.head()\n",
        "df_score.to_csv('test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxaM9uUmvMzi"
      },
      "source": [
        "# Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw54fzzcvOLg"
      },
      "source": [
        "# Limitations of the Approach"
      ]
    }
  ]
}