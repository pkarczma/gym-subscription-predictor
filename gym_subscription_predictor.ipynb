{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_subscription_predictor.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCfIqN1uuzIW"
      },
      "source": [
        "# Executive Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZXrkeP7u_Ns"
      },
      "source": [
        "# Input Data and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_p7j_XmsYmm"
      },
      "source": [
        "### Access data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgYjTYBeCXhn"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj27OvRMxenD"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mvKSUs_Ca3z"
      },
      "source": [
        "Clone files stored in the git repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhctCVOi0JIU"
      },
      "source": [
        "!git clone https://pkarczma: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@github.com/pkarczma/gym-subscription-predictor.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPSHXrj2CeRK"
      },
      "source": [
        "Read CSV and JSON files with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YudIQBVaw3OS"
      },
      "source": [
        "path = 'gym-subscription-predictor/'\n",
        "df_csv = pd.read_csv(path+'train.csv')\n",
        "df_json = pd.read_json(path+'train.json')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1I_NrSsiLX"
      },
      "source": [
        "### Analyse and clear data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VTw4XZCoFD"
      },
      "source": [
        "Get familiar with the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33u8oZZk_BMe"
      },
      "source": [
        "df_csv.info()\n",
        "df_csv.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ED8BDNPkGK"
      },
      "source": [
        "There are some columns that seem unnnecessary for our model. We will drop them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Var-YiBKPAwQ"
      },
      "source": [
        "df_csv = df_csv.drop(columns=['name', 'location_population', 'location_from_population', 'daily_commute', 'credit_card_type'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NM9TAI6EPtI"
      },
      "source": [
        "Count the number of missing values in each of the remaining columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ4C8MHODTMO"
      },
      "source": [
        "df_csv.isnull().sum(axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYdofSPOC8dM"
      },
      "source": [
        "There are some NaN values in data in several columns. We need to use a different approach depending on the column with the missing values. The following procedure will be applied:\n",
        "* 'user_id' / 'target' / 'location' / 'occupation' / 'friends_number': no missing values, columns are useful, nothing changes\n",
        "* 'name' / 'location_population' / 'location_from' / 'location_from_population': a few missing values, this column isn't necessary for model prediction so it will be dropped\n",
        "* 'education': fill missing falues with a median of a column\n",
        "* 'hobbies': fill missing values with empty string\n",
        "\n",
        "For the remaining data with missing values it is problematic to replace it. Thus, the rows with at least one missing calue will be dropped from the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAcLq_a0_Rtr"
      },
      "source": [
        "df_csv['hobbies'] = df_csv['hobbies'].fillna('')\n",
        "df_csv['education'] = df_csv['education'].fillna(df_csv['education'].median())\n",
        "df_csv = df_csv.dropna()\n",
        "df_csv.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDV-Kfvpk8Dj"
      },
      "source": [
        "As a result, we removed around 25% of all rows, but now the data is clean and ready for the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDtd4CLXr5T0"
      },
      "source": [
        "### Transform data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5LNwLGsE-B"
      },
      "source": [
        "In order to prepare data for the model we need to convert it to the proper format. The following code will convert data to categories so that is it easier for the model to read it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnHh-uOtLFq"
      },
      "source": [
        "df_csv['sex'] = df_csv['sex'].astype('category').cat.codes\n",
        "df_csv['location'] = df_csv['location'].astype('category').cat.codes\n",
        "df_csv['location_from'] = df_csv['location_from'].astype('category').cat.codes\n",
        "df_csv['occupation'] = df_csv['occupation'].astype('category').cat.codes\n",
        "df_csv['relationship_status'] = df_csv['relationship_status'].astype('category').cat.codes"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbhk_WEBLkyK"
      },
      "source": [
        "For the date of birth, I assume there is no need to keep the exact date - having just a year of birth should be enough for the model. I will drop the day and month information from 'dob' column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnV7mBe_L1YY"
      },
      "source": [
        "df_csv['dob'] = pd.DatetimeIndex(df_csv['dob']).year"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ybXqKhru2E0"
      },
      "source": [
        "For the 'hobbies' column the best way is to get dummies for each value and split it into several columns with numbers 0 and 1 indicating interest (or lack of interest) in a particular hobby. An additional 'hobby_' prefix will indicate that this column represents a hobby, but also to make sure that none of the column names are overlapping with the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsfVaCK7pJwi"
      },
      "source": [
        "df_csv = pd.concat([df_csv.drop('hobbies', axis=1), df_csv['hobbies'].str.get_dummies(sep=',').add_prefix('hobby_')], axis=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEBuPt4fvZoZ"
      },
      "source": [
        "At this point the data contain only numbers, there are no missing values, and it is prepared for the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KuLSWfovvSj"
      },
      "source": [
        "df_csv.info()\n",
        "df_csv.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN7OlPw_vFjh"
      },
      "source": [
        "# Model Selection and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPSUlIUzXcf"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Ozjnawyc97"
      },
      "source": [
        "For the model training and testing the dataset will be split into two subsets:\n",
        "* 80% of the data will be used for training\n",
        "* 20% od the remaining data will be used for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVIGMXCx3Og"
      },
      "source": [
        "train_dataset = df_csv.sample(frac=0.8, random_state=0)\n",
        "test_dataset = df_csv.drop(train_dataset.index)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46_y2c4y2x7"
      },
      "source": [
        "Afterwards, let's separate labels from features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EGNKJrEzLTw"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "train_labels = train_features.pop('target')\n",
        "test_labels = test_features.pop('target')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhDAaP8uz31-"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIMu-S6Pz6Tr"
      },
      "source": [
        "Now it is time to build a model. For this task I am going to use Keras interface from the TensorFlow library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTr6qm600tfd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaCPssnG1noS"
      },
      "source": [
        "The selected model will be a regression-based neural network consisting of several input, hidden, and output layers. It will use existing data prepared in the previous section as an input in order to create predictions of the desired variable.\n",
        "\n",
        "This model prefers to have the input data normalized in a specific way. Thus, we need to create a normalization layer that is adapted to the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9aKQTJD22Mu"
      },
      "source": [
        "normalizer = layers.experimental.preprocessing.Normalization()\n",
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXO3e9XE3rCj"
      },
      "source": [
        "Afterwards, we can build a model consisting of a sequential stack of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YdhwPIh4UmI"
      },
      "source": [
        "train_features.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaaF8Rf537FB"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(183, input_dim=183, activation='relu'),\n",
        "    layers.Dense(60, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmL5utb5kDg"
      },
      "source": [
        "### Compile and fit model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uf79bLo5IWr"
      },
      "source": [
        "Next step is to compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adPXX-CJ5S_V"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISqfsuHz5bpC"
      },
      "source": [
        "Then, we can fit the model providing different settings that can be adjusted for the model efficacy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zscy8dS56Ces"
      },
      "source": [
        "history = model.fit(\n",
        "    # Data to be used for training\n",
        "    train_features, train_labels,\n",
        "    # Number of epochs\n",
        "    epochs=10,\n",
        "    # Suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSiw_3tm6f4u"
      },
      "source": [
        "We can have a look at the last few epochs of the training of the model in order to see if everything works well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuKOm3rE6rkP"
      },
      "source": [
        "# Show history in the last few epochs\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQAyb8qnvJvn"
      },
      "source": [
        "# Model Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q1QhngttOEm"
      },
      "source": [
        "test_predictions = model.predict(test_features)\n",
        "good = 0\n",
        "all = 0\n",
        "for i, j in zip(np.around(test_predictions), test_labels):\n",
        "  if i == j:\n",
        "    good += 1\n",
        "  all += 1\n",
        "print(good / all)\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predictions')\n",
        "lims = [-0.1, 1.1]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxaM9uUmvMzi"
      },
      "source": [
        "# Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw54fzzcvOLg"
      },
      "source": [
        "# Limitations of the Approach"
      ]
    }
  ]
}